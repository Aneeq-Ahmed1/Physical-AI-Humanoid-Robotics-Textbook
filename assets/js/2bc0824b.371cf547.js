"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2934],{933:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"humanoid-robotics-textbook/data-model","title":"Textbook Data Model","description":"This document outlines the key entities and their relationships as conceptualized for the \\"Physical AI & Humanoid Robotics Textbook Project.\\" These entities represent the fundamental concepts and components discussed throughout the textbook.","source":"@site/docs/humanoid-robotics-textbook/data-model.md","sourceDirName":"humanoid-robotics-textbook","slug":"/humanoid-robotics-textbook/data-model","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/humanoid-robotics-textbook/data-model","draft":false,"unlisted":false,"editUrl":"https://github.com/aneeq-ai/Humanoid-Robotics-Textbook/edit/master/my-website/docs/humanoid-robotics-textbook/data-model.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Textbook Data Model"}}');var o=i(4848),t=i(8453);const r={sidebar_position:4,title:"Textbook Data Model"},l="Data Model: Physical AI & Humanoid Robotics Textbook Project",c={},a=[{value:"Key Entities",id:"key-entities",level:2},{value:"Robot Model",id:"robot-model",level:3},{value:"ROS 2 Node",id:"ros-2-node",level:3},{value:"Simulation Environment",id:"simulation-environment",level:3},{value:"Perception Pipeline",id:"perception-pipeline",level:3},{value:"Navigation Pipeline",id:"navigation-pipeline",level:3},{value:"Manipulation Pipeline",id:"manipulation-pipeline",level:3},{value:"LLM (Large Language Model)",id:"llm-large-language-model",level:3},{value:"VLA (Vision-Language-Action)",id:"vla-vision-language-action",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"data-model-physical-ai--humanoid-robotics-textbook-project",children:"Data Model: Physical AI & Humanoid Robotics Textbook Project"})}),"\n",(0,o.jsx)(n.p,{children:'This document outlines the key entities and their relationships as conceptualized for the "Physical AI & Humanoid Robotics Textbook Project." These entities represent the fundamental concepts and components discussed throughout the textbook.'}),"\n",(0,o.jsx)(n.h2,{id:"key-entities",children:"Key Entities"}),"\n",(0,o.jsx)(n.h3,{id:"robot-model",children:"Robot Model"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": Representation of a physical humanoid robot in simulation. This entity encapsulates the robot's kinematics, dynamics, sensor configurations, and actuator properties."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"robot_name"}),": Unique identifier for the robot model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"urdf_path"}),": Path to the Unified Robot Description Format (URDF) file defining the robot's structure."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"joints"}),": Collection of robotic joints with properties like limits, types, and current positions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"links"}),": Collection of rigid bodies (links) comprising the robot."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"sensors"}),": Configuration of various sensors (e.g., cameras, lidar, IMU) attached to the robot, including their type, placement, and data output formats."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"actuators"}),": Definition of motors or other actuation mechanisms for controlling robot movement."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-node",children:"ROS 2 Node"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": An independent executable processing unit in ROS 2. Nodes communicate with each other using topics, services, and actions. This entity represents the software components that perform specific tasks within the robot's control system."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"node_name"}),": Unique name for the ROS 2 node."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"executables"}),": The code responsible for the node's logic."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"topics_published"}),": List of ROS 2 topics this node publishes data to, including message types."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"topics_subscribed"}),": List of ROS 2 topics this node subscribes to, including message types."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"services_provided"}),": List of ROS 2 services this node offers."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"services_used"}),": List of ROS 2 services this node calls."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"parameters"}),": Configuration parameters for the node."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"simulation-environment",children:"Simulation Environment"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": The virtual space where robot models are tested and behaviors are developed without relying on physical hardware. This includes platforms like Gazebo and Unity."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"environment_name"}),': Identifier for the simulation environment (e.g., "Gazebo", "Unity").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"terrain_properties"}),": Characteristics of the simulated ground (friction, texture)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"objects_present"}),": List of static and dynamic objects within the environment, including their models and positions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"physics_engine_config"}),": Configuration settings for the underlying physics engine (e.g., gravity, time steps, solvers)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rendering_settings"}),": Visual fidelity settings (e.g., lighting, shadows, textures)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": Components for processing sensor data to understand the environment. This typically involves processing visual data from cameras, depth sensors, and lidar to detect objects, map the environment, and track motion."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"pipeline_name"}),": Name of the perception pipeline."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"sensor_inputs"}),": Types of sensors providing data (e.g., RGB camera, depth camera, lidar)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"algorithms"}),": Machine learning models or classical algorithms used for processing (e.g., object detection, segmentation, SLAM)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"outputs"}),": Types of information generated (e.g., object bounding boxes, semantic maps, pose estimates)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"performance_metrics"}),": Latency, accuracy, processing throughput."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"navigation-pipeline",children:"Navigation Pipeline"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": Components for path planning and movement control. This pipeline enables the robot to move from one point to another, avoiding obstacles, and reaching specified goals."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"pipeline_name"}),": Name of the navigation pipeline."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"input_data"}),": Sensor data (e.g., lidar scans, camera images, odometry) and target goals."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"localization_module"}),": Estimates the robot's position and orientation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"mapping_module"}),": Builds or updates an environmental map."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"path_planner"}),": Generates a global and local path to the target."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"controller"}),": Executes motor commands to follow the planned path."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"obstacle_avoidance_system"}),": Handles dynamic obstacle detection and avoidance."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"manipulation-pipeline",children:"Manipulation Pipeline"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": Components for robotic arm/hand control for object interaction. This pipeline manages grasping, moving, and placing objects."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"pipeline_name"}),": Name of the manipulation pipeline."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"target_object_data"}),": Information about the object to be manipulated (e.g., pose, geometry, type)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"inverse_kinematics_solver"}),": Calculates joint angles required to reach a target pose."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"trajectory_planner"}),": Generates smooth, collision-free arm movements."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"gripper_controller"}),": Manages the opening and closing of the robot's end-effector."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"force_feedback_system"}),": Uses force/torque sensors for compliant manipulation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"llm-large-language-model",children:"LLM (Large Language Model)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": An AI model for natural language understanding and generation, used here for interpreting human commands and generating high-level task plans for the robot."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"model_name"}),": Identifier for the specific LLM being used."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"input_modalities"}),": Text, speech (after ASR)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"output_format"}),": Natural language responses, structured action plans (e.g., JSON, YAML)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"context_window_size"}),": Amount of historical conversation/data the LLM can process."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"fine_tuning_data"}),": Specific datasets used to adapt the LLM for robotics tasks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"vla-vision-language-action",children:"VLA (Vision-Language-Action)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": A framework integrating visual perception, language understanding, and robotic actions. This entity represents the overarching system that enables a robot to interpret multimodal commands and execute complex tasks."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Attributes (Conceptual)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"framework_name"}),": Identifier for the VLA framework."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"vision_module"}),": Integration point for perception pipeline outputs."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"language_module"}),": Integration point for LLM/ASR outputs."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"action_module"}),": Integration point for manipulation/navigation pipelines."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"multimodal_fusion_strategy"}),": How visual and language data are combined for reasoning."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"task_planner"}),": Component responsible for breaking down high-level language commands into executable sub-tasks."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"error_handling_mechanisms"}),": Strategies for dealing with interpretation errors or execution failures."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);