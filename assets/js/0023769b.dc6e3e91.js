"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8764],{4348:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"humanoid-robotics-textbook/conceptual-interfaces","title":"Textbook Conceptual Interfaces","description":"This document outlines the conceptual interfaces and interaction points between various modules and systems discussed in the \\"Physical AI & Humanoid Robotics Textbook Project.\\" These interfaces facilitate understanding of how different components communicate and integrate to form a complete humanoid robotics system.","source":"@site/docs/humanoid-robotics-textbook/conceptual-interfaces.md","sourceDirName":"humanoid-robotics-textbook","slug":"/humanoid-robotics-textbook/conceptual-interfaces","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/humanoid-robotics-textbook/conceptual-interfaces","draft":false,"unlisted":false,"editUrl":"https://github.com/aneeq-ai/Humanoid-Robotics-Textbook/edit/master/my-website/docs/humanoid-robotics-textbook/conceptual-interfaces.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Textbook Conceptual Interfaces"}}');var t=i(4848),o=i(8453);const r={sidebar_position:5,title:"Textbook Conceptual Interfaces"},c="Conceptual Interfaces: Physical AI & Humanoid Robotics Textbook Project",a={},l=[{value:"1. ROS 2 Interfaces (Robotic Nervous System)",id:"1-ros-2-interfaces-robotic-nervous-system",level:2},{value:"1.1 Topics (Publisher/Subscriber)",id:"11-topics-publishersubscriber",level:3},{value:"1.2 Services (Client/Server)",id:"12-services-clientserver",level:3},{value:"1.3 Actions (Client/Server - Goal/Feedback/Result)",id:"13-actions-clientserver---goalfeedbackresult",level:3},{value:"2. Simulation Environment Interfaces (Digital Twin Simulation)",id:"2-simulation-environment-interfaces-digital-twin-simulation",level:2},{value:"2.1 Robot Model Loading &amp; Configuration",id:"21-robot-model-loading--configuration",level:3},{value:"2.2 Physics Engine Interaction",id:"22-physics-engine-interaction",level:3},{value:"2.3 Sensor Data Generation",id:"23-sensor-data-generation",level:3},{value:"3. NVIDIA Isaac Interfaces (AI-Robot Brain)",id:"3-nvidia-isaac-interfaces-ai-robot-brain",level:2},{value:"3.1 Perception Pipeline API",id:"31-perception-pipeline-api",level:3},{value:"3.2 Navigation Pipeline API",id:"32-navigation-pipeline-api",level:3},{value:"3.3 Manipulation Pipeline API",id:"33-manipulation-pipeline-api",level:3},{value:"4. LLM/VLA Interfaces (Vision-Language-Action)",id:"4-llmvla-interfaces-vision-language-action",level:2},{value:"4.1 Speech-to-Text (ASR - e.g., Whisper) Interface",id:"41-speech-to-text-asr---eg-whisper-interface",level:3},{value:"4.2 LLM / VLM (Vision-Language Model) Interface",id:"42-llm--vlm-vision-language-model-interface",level:3},{value:"4.3 Action Execution Interface (from LLM to Robot Control)",id:"43-action-execution-interface-from-llm-to-robot-control",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"conceptual-interfaces-physical-ai--humanoid-robotics-textbook-project",children:"Conceptual Interfaces: Physical AI & Humanoid Robotics Textbook Project"})}),"\n",(0,t.jsx)(n.p,{children:'This document outlines the conceptual interfaces and interaction points between various modules and systems discussed in the "Physical AI & Humanoid Robotics Textbook Project." These interfaces facilitate understanding of how different components communicate and integrate to form a complete humanoid robotics system.'}),"\n",(0,t.jsx)(n.h2,{id:"1-ros-2-interfaces-robotic-nervous-system",children:"1. ROS 2 Interfaces (Robotic Nervous System)"}),"\n",(0,t.jsx)(n.p,{children:"ROS 2 provides a flexible communication framework for distributed robotics components. The textbook will detail the use of:"}),"\n",(0,t.jsx)(n.h3,{id:"11-topics-publishersubscriber",children:"1.1 Topics (Publisher/Subscriber)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Asynchronous, one-to-many communication for streaming data."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/robot/joint_states"})," (sensor_msgs/JointState): Robot's current joint positions, velocities, efforts."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/camera/image_raw"})," (sensor_msgs/Image): Raw image data from a camera sensor."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/lidar/scan"})," (sensor_msgs/LaserScan): Laser scan data from a LiDAR sensor."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/cmd_vel"})," (geometry_msgs/Twist): Velocity commands for robot movement (e.g., base mobility)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/perception/object_detections"})," (vision_msgs/Detection2DArray): Bounding box detections of objects."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"12-services-clientserver",children:"1.2 Services (Client/Server)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Synchronous, request-response communication for discrete actions or queries."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/robot/set_joint_position"})," (std_srvs/SetFloat): Request to set a specific joint to a target position."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/navigation/get_path"})," (nav_msgs/GetPlan): Request for a path between two points."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/manipulation/grasp_object"})," (custom_interfaces/GraspObject): Request to grasp a specified object, returning success/failure."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"13-actions-clientserver---goalfeedbackresult",children:"1.3 Actions (Client/Server - Goal/Feedback/Result)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Long-running, pre-emptable tasks with continuous feedback."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/robot/navigate_to_pose"})," (nav_msgs/NavigateToPose): Goal to move to a target pose, with feedback on current progress and final result."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"/manipulation/pick_and_place"})," (custom_interfaces/PickAndPlace): Goal to pick an object from a source and place it at a destination, with feedback on intermediate steps."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"2-simulation-environment-interfaces-digital-twin-simulation",children:"2. Simulation Environment Interfaces (Digital Twin Simulation)"}),"\n",(0,t.jsx)(n.p,{children:"Interaction between robot models and simulation platforms (Gazebo, Unity) via:"}),"\n",(0,t.jsx)(n.h3,{id:"21-robot-model-loading--configuration",children:"2.1 Robot Model Loading & Configuration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Method"}),": Loading URDF (Unified Robot Description Format) or SDF (Simulation Description Format) files into the simulator."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parameters"}),": Robot geometry, joint limits, inertial properties, sensor attachments, initial pose."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": Simulated robot instance with physics and sensor models."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"22-physics-engine-interaction",children:"2.2 Physics Engine Interaction"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Applied forces, torques, joint commands (position, velocity, effort)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Updated joint states, link poses, contact forces."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control"}),": Direct control of joint actuators, or integration with external controllers (e.g., ROS 2 controllers)."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"23-sensor-data-generation",children:"2.3 Sensor Data Generation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Method"}),": Simulator-provided APIs for camera images, depth maps, lidar scans, IMU data, force/torque sensor readings."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parameters"}),": Sensor type, resolution, noise models, update rates, field of view."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": Synthesized sensor data stream, often published as ROS 2 topics."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"3-nvidia-isaac-interfaces-ai-robot-brain",children:"3. NVIDIA Isaac Interfaces (AI-Robot Brain)"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac (e.g., Isaac Sim, Isaac Gym) provides a platform for developing AI-driven robotics applications."}),"\n",(0,t.jsx)(n.h3,{id:"31-perception-pipeline-api",children:"3.1 Perception Pipeline API"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Simulated or real camera images, depth maps, point clouds."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Object detections (bounding boxes, classes), instance segmentation masks, pose estimations, semantic maps."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),": ",(0,t.jsx)(n.code,{children:"detect_objects(image_data)"}),", ",(0,t.jsx)(n.code,{children:"segment_scene(point_cloud)"}),", ",(0,t.jsx)(n.code,{children:"estimate_pose(target_object_id)"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"32-navigation-pipeline-api",children:"3.2 Navigation Pipeline API"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Current robot pose, target pose/goal, environmental map (occupancy grid, semantic map)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Trajectory plan (sequence of waypoints), velocity commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),": ",(0,t.jsx)(n.code,{children:"plan_path(start_pose, goal_pose, map_data)"}),", ",(0,t.jsx)(n.code,{children:"update_costmap(sensor_data)"}),", ",(0,t.jsx)(n.code,{children:"get_velocity_commands(current_pose, desired_path)"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"33-manipulation-pipeline-api",children:"3.3 Manipulation Pipeline API"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Target object pose, robot arm state, gripper state."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Joint commands (position, velocity, torque), gripper commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),": ",(0,t.jsx)(n.code,{children:"plan_grasp(object_pose)"}),", ",(0,t.jsx)(n.code,{children:"execute_trajectory(joint_trajectory)"}),", ",(0,t.jsx)(n.code,{children:"control_gripper(command)"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"4-llmvla-interfaces-vision-language-action",children:"4. LLM/VLA Interfaces (Vision-Language-Action)"}),"\n",(0,t.jsx)(n.p,{children:"Integration of Large Language Models and Vision-Language-Action frameworks."}),"\n",(0,t.jsx)(n.h3,{id:"41-speech-to-text-asr---eg-whisper-interface",children:"4.1 Speech-to-Text (ASR - e.g., Whisper) Interface"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Raw audio stream from microphone."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Text transcript of spoken commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),": ",(0,t.jsx)(n.code,{children:"transcribe_audio(audio_buffer)"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"42-llm--vlm-vision-language-model-interface",children:"4.2 LLM / VLM (Vision-Language Model) Interface"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Text commands (from ASR), visual features (from perception pipeline), current robot state, task context."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": High-level action plans (structured text or JSON), clarified questions, natural language responses."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"generate_task_plan(text_command, visual_context, robot_state)"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"answer_query(text_query, visual_context, robot_state)"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"clarify_instruction(ambiguous_command)"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"43-action-execution-interface-from-llm-to-robot-control",children:"4.3 Action Execution Interface (from LLM to Robot Control)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": Structured action plan (e.g., sequence of sub-tasks like ",(0,t.jsx)(n.code,{children:"{'action': 'grasp', 'object': 'red cube'}"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Outputs"}),": Calls to underlying navigation, manipulation, or other low-level robot control APIs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Methods"}),": ",(0,t.jsx)(n.code,{children:"execute_action_plan(plan)"}),", ",(0,t.jsx)(n.code,{children:"map_high_level_to_low_level_command(high_level_action)"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>c});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);